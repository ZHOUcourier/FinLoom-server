"""
æ¨¡å‹æ¨ç†å¯è§†åŒ–æ¨¡å—
æä¾›ç±»ä¼¼ Ollama çš„ä¸“ä¸šæ¨¡å‹æ¨ç†è¿›åº¦æ˜¾ç¤ºï¼ŒåŒ…æ‹¬é€Ÿç‡ã€è¿›åº¦æ¡å’Œé¢„ä¼°æ—¶é—´
"""

import queue
import time
from datetime import timedelta
from threading import Thread
from typing import Any, Dict, Optional

try:
    from rich.console import Console
    from rich.live import Live
    from rich.panel import Panel
    from rich.progress import (
        BarColumn,
        MofNCompleteColumn,
        Progress,
        SpinnerColumn,
        TaskProgressColumn,
        TextColumn,
        TimeRemainingColumn,
    )
    from rich.table import Table

    HAS_RICH = True
except ImportError:
    HAS_RICH = False

try:
    from transformers import TextIteratorStreamer

    HAS_TRANSFORMERS = True
except ImportError:
    HAS_TRANSFORMERS = False

from common.logging_system import setup_logger

logger = setup_logger("model_inference_visualizer")


class ModelInferenceVisualizer:
    """æ¨¡å‹æ¨ç†å¯è§†åŒ–å™¨ - ç±»ä¼¼ Ollama çš„ä¸“ä¸šè¿›åº¦æ˜¾ç¤º"""

    def __init__(self):
        """åˆå§‹åŒ–å¯è§†åŒ–å™¨"""
        self.console = Console() if HAS_RICH else None
        self.start_time = None
        self.token_count = 0
        self.last_token_time = None

    def create_progress_display(self) -> Optional[Progress]:
        """åˆ›å»ºè¿›åº¦æ¡æ˜¾ç¤º

        Returns:
            Rich Progress å¯¹è±¡
        """
        if not HAS_RICH:
            return None

        return Progress(
            SpinnerColumn(),
            TextColumn("[bold blue]{task.description}"),
            BarColumn(complete_style="cyan", finished_style="green"),
            TaskProgressColumn(),
            "â€¢",
            TextColumn("[cyan]{task.fields[speed]}"),
            "â€¢",
            TimeRemainingColumn(),
            console=self.console,
        )

    def create_stats_table(
        self,
        tokens_generated: int,
        tokens_per_sec: float,
        elapsed_time: float,
        eta: Optional[float] = None,
    ) -> Table:
        """åˆ›å»ºç»Ÿè®¡ä¿¡æ¯è¡¨æ ¼

        Args:
            tokens_generated: å·²ç”Ÿæˆçš„ token æ•°
            tokens_per_sec: ç”Ÿæˆé€Ÿç‡ (tokens/s)
            elapsed_time: å·²ç”¨æ—¶é—´
            eta: é¢„ä¼°å‰©ä½™æ—¶é—´

        Returns:
            Rich Table å¯¹è±¡
        """
        if not HAS_RICH:
            return None

        table = Table(show_header=False, box=None, padding=(0, 1))
        table.add_column("Metric", style="cyan")
        table.add_column("Value", style="white")

        table.add_row("ğŸ¯ Tokens", f"{tokens_generated}")
        table.add_row("âš¡ Speed", f"{tokens_per_sec:.2f} tokens/s")
        table.add_row("â±ï¸  Elapsed", f"{elapsed_time:.2f}s")

        if eta is not None:
            table.add_row("â³ ETA", f"{eta:.1f}s")

        return table

    def display_loading_model(self, model_name: str = "FIN-R1"):
        """æ˜¾ç¤ºæ¨¡å‹åŠ è½½ç•Œé¢

        Args:
            model_name: æ¨¡å‹åç§°
        """
        if not HAS_RICH:
            logger.info(f"Loading {model_name} model...")
            return

        with Progress(
            SpinnerColumn(),
            TextColumn("[bold cyan]{task.description}"),
            console=self.console,
        ) as progress:
            task = progress.add_task(f"[cyan]Loading {model_name} model...", total=None)
            # è¿™ä¸ªå‡½æ•°ç«‹å³è¿”å›ï¼Œå®é™…åŠ è½½åœ¨è°ƒç”¨å¤„è¿›è¡Œ

    def create_inference_streamer(
        self, tokenizer, max_new_tokens: int = 100, skip_prompt: bool = True
    ):
        """åˆ›å»ºæµå¼æ¨ç†å™¨

        Args:
            tokenizer: åˆ†è¯å™¨
            max_new_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
            skip_prompt: æ˜¯å¦è·³è¿‡è¾“å…¥æç¤ºè¯

        Returns:
            TextIteratorStreamer å¯¹è±¡å’Œç”Ÿæˆçº¿ç¨‹
        """
        if not HAS_TRANSFORMERS:
            logger.warning("Transformers not available, cannot create streamer")
            return None, None

        streamer = TextIteratorStreamer(
            tokenizer, skip_prompt=skip_prompt, skip_special_tokens=True
        )

        return streamer

    def visualize_generation(
        self, streamer, max_new_tokens: int, model_name: str = "FIN-R1", timeout: int = 30
    ) -> str:
        """å¯è§†åŒ–æ¨¡å‹ç”Ÿæˆè¿‡ç¨‹

        Args:
            streamer: TextIteratorStreamer å¯¹è±¡
            max_new_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
            model_name: æ¨¡å‹åç§°
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œå¦‚æœè¶…æ—¶ä¸”æ²¡æœ‰ç”Ÿæˆä»»ä½•tokenåˆ™æŠ›å‡ºå¼‚å¸¸

        Returns:
            ç”Ÿæˆçš„å®Œæ•´æ–‡æœ¬
            
        Raises:
            TimeoutError: å¦‚æœè¶…æ—¶ä¸”æ²¡æœ‰ç”Ÿæˆä»»ä½•token
        """
        if not HAS_RICH or streamer is None:
            # Fallback: ç®€å•çš„æ–‡æœ¬è¾“å‡º
            generated_text = ""
            logger.info("âš¡ Streaming generation (simple mode)...")
            start = time.time()
            try:
                for text in streamer:
                    generated_text += text
                    print(text, end="", flush=True)
                    # æ£€æŸ¥è¶…æ—¶
                    if time.time() - start > timeout and len(generated_text) == 0:
                        logger.error(f"â±ï¸ Timeout: {timeout}s elapsed with 0 tokens")
                        raise TimeoutError(f"Generation timeout after {timeout}s with no tokens")
            except Exception as e:
                logger.error(f"Streaming error: {e}")
                if isinstance(e, TimeoutError):
                    raise
            print()
            return generated_text

        generated_text = ""
        self.start_time = time.time()
        self.token_count = 0
        last_update_time = self.start_time
        timeout_checked = False

        with Progress(
            SpinnerColumn(),
            TextColumn("[bold blue]{task.description}"),
            BarColumn(complete_style="cyan", finished_style="green"),
            MofNCompleteColumn(),
            "â€¢",
            TextColumn("[cyan]{task.fields[speed]}"),
            "â€¢",
            TimeRemainingColumn(),
            console=self.console,
            transient=False,  # ä¿æŒè¿›åº¦æ¡æ˜¾ç¤º
        ) as progress:
            task = progress.add_task(
                f"[cyan]ğŸ¤– {model_name} Generating",
                total=max_new_tokens,
                speed="0.00 tok/s",
            )

            try:
                # ğŸ”‘ å…³é”®ï¼šä½¿ç”¨ try-except æ•è·è¶…æ—¶é”™è¯¯
                for text in streamer:
                    generated_text += text
                    self.token_count += 1

                    # æ›´æ–°è¿›åº¦ï¼ˆé™åˆ¶æ›´æ–°é¢‘ç‡ï¼Œé¿å…è¿‡äºé¢‘ç¹ï¼‰
                    current_time = time.time()
                    if current_time - last_update_time >= 0.1:  # æ¯100msæ›´æ–°ä¸€æ¬¡
                        elapsed = current_time - self.start_time
                        tokens_per_sec = (
                            self.token_count / elapsed if elapsed > 0 else 0
                        )

                        progress.update(
                            task,
                            completed=self.token_count,
                            speed=f"{tokens_per_sec:.2f} tok/s",
                        )
                        last_update_time = current_time
                        
                        # ğŸ”‘ æ£€æŸ¥è¶…æ—¶ï¼šå¦‚æœè¶…è¿‡æŒ‡å®šæ—¶é—´ä¸”æ²¡æœ‰ç”Ÿæˆä»»ä½•token
                        if elapsed > timeout and self.token_count == 0 and not timeout_checked:
                            timeout_checked = True
                            logger.error(f"â±ï¸ Timeout: {timeout}s elapsed with 0 tokens")
                            self.console.print(
                                f"[red]âŒ Generation timeout: {timeout}s elapsed with no tokens[/red]"
                            )
                            raise TimeoutError(f"Generation timeout after {timeout}s with no tokens")

                # æœ€åæ›´æ–°åˆ°å®é™…å®Œæˆçš„æ•°é‡
                elapsed = time.time() - self.start_time
                tokens_per_sec = self.token_count / elapsed if elapsed > 0 else 0
                progress.update(
                    task,
                    completed=self.token_count,
                    speed=f"{tokens_per_sec:.2f} tok/s",
                )

            except StopIteration:
                logger.info("âœ… Generation completed normally")
            except TimeoutError:
                # ğŸ”‘ æ£€æŸ¥æ˜¯å¦æ˜¯0 tokenè¶…æ—¶
                elapsed = time.time() - self.start_time
                if self.token_count == 0 and elapsed > timeout:
                    logger.error(f"âŒ FIN-R1 timeout with 0 tokens after {elapsed:.1f}s - will switch to Aliyun")
                    self.console.print("[red]âŒ Model timeout with 0 tokens - switching to backup[/red]")
                    raise  # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œè®©ä¸Šå±‚åˆ‡æ¢åˆ°é˜¿é‡Œäº‘
                else:
                    logger.warning("âš ï¸ Generation timeout - streamer timed out")
                    self.console.print("[yellow]âš ï¸ Generation timeout[/yellow]")
            except Exception as e:
                logger.error(f"âŒ Generation error: {e}")
                self.console.print(f"[red]âŒ Error: {e}[/red]")
                # å¦‚æœæ˜¯è¶…æ—¶ç›¸å…³çš„å¼‚å¸¸ï¼Œé‡æ–°æŠ›å‡º
                if "timeout" in str(e).lower() or isinstance(e, TimeoutError):
                    raise

        # ğŸ”‘ æœ€ç»ˆæ£€æŸ¥ï¼šå¦‚æœæ•´ä¸ªè¿‡ç¨‹å®Œæˆä½†æ²¡æœ‰ç”Ÿæˆä»»ä½•token
        elapsed = time.time() - self.start_time
        if self.token_count == 0 and elapsed > timeout:
            logger.error(f"âŒ FIN-R1 generated 0 tokens after {elapsed:.1f}s")
            self.console.print(
                "[red]âŒ Model generated no tokens - switching to backup service[/red]"
            )
            raise TimeoutError(f"Model generated 0 tokens after {elapsed:.1f}s")

        # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
        tokens_per_sec = self.token_count / elapsed if elapsed > 0 else 0

        self.console.print()

        if self.token_count > 0:
            self.console.print(
                Panel(
                    self.create_stats_table(self.token_count, tokens_per_sec, elapsed),
                    title="[bold green]âœ… Generation Complete",
                    border_style="green",
                )
            )
        else:
            self.console.print(
                "[yellow]âš ï¸ No tokens generated - check if model is working properly[/yellow]"
            )

        return generated_text

    def simple_progress_bar(self, description: str, total: int, callback=None):
        """ç®€å•çš„è¿›åº¦æ¡ï¼ˆç”¨äºéæµå¼ç”Ÿæˆï¼‰

        Args:
            description: ä»»åŠ¡æè¿°
            total: æ€»æ­¥éª¤æ•°
            callback: æ‰§è¡Œçš„å›è°ƒå‡½æ•°
        """
        if not HAS_RICH:
            logger.info(f"{description}...")
            if callback:
                return callback()
            return

        with Progress(
            SpinnerColumn(),
            TextColumn("[bold blue]{task.description}"),
            BarColumn(),
            TaskProgressColumn(),
            console=self.console,
        ) as progress:
            task = progress.add_task(description, total=total)

            if callback:
                result = callback(progress, task)
                progress.update(task, completed=total)
                return result


class InferenceProgressCallback:
    """æ¨ç†è¿›åº¦å›è°ƒç±» - ç”¨äºéæµå¼ç”Ÿæˆçš„è¿›åº¦ç›‘æ§"""

    def __init__(
        self, max_new_tokens: int, visualizer: Optional[ModelInferenceVisualizer] = None
    ):
        """åˆå§‹åŒ–å›è°ƒ

        Args:
            max_new_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
            visualizer: å¯è§†åŒ–å™¨å®ä¾‹
        """
        self.max_new_tokens = max_new_tokens
        self.visualizer = visualizer or ModelInferenceVisualizer()
        self.start_time = None
        self.current_tokens = 0

    def __call__(self, *args, **kwargs):
        """å›è°ƒå‡½æ•°"""
        if self.start_time is None:
            self.start_time = time.time()

        self.current_tokens += 1
        elapsed = time.time() - self.start_time
        tokens_per_sec = self.current_tokens / elapsed if elapsed > 0 else 0

        # æ›´æ–°è¿›åº¦æ˜¾ç¤º
        if self.visualizer and self.visualizer.console:
            progress_pct = (self.current_tokens / self.max_new_tokens) * 100
            self.visualizer.console.print(
                f"âš¡ Progress: {progress_pct:.1f}% | "
                f"Speed: {tokens_per_sec:.2f} tok/s | "
                f"Tokens: {self.current_tokens}/{self.max_new_tokens}",
                end="\r",
            )


def display_model_info(model_name: str, model_size: str = "Unknown"):
    """æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯é¢æ¿

    Args:
        model_name: æ¨¡å‹åç§°
        model_size: æ¨¡å‹å¤§å°
    """
    if not HAS_RICH:
        logger.info(f"Model: {model_name} ({model_size})")
        return

    console = Console()

    table = Table(show_header=False, box=None)
    table.add_column("Property", style="cyan")
    table.add_column("Value", style="white")

    table.add_row("ğŸ¤– Model", model_name)
    table.add_row("ğŸ“¦ Size", model_size)
    table.add_row("âš™ï¸  Status", "[green]Ready[/green]")

    console.print(
        Panel(table, title="[bold cyan]Model Information", border_style="cyan")
    )


# ä¾¿æ·å‡½æ•°
def create_visualizer() -> ModelInferenceVisualizer:
    """åˆ›å»ºå¯è§†åŒ–å™¨å®ä¾‹

    Returns:
        ModelInferenceVisualizer å®ä¾‹
    """
    return ModelInferenceVisualizer()


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    import asyncio

    visualizer = ModelInferenceVisualizer()

    # æµ‹è¯•1: æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
    display_model_info("FIN-R1", "7B")

    # æµ‹è¯•2: æ¨¡æ‹Ÿç”Ÿæˆè¿‡ç¨‹
    print("\n" + "=" * 50)
    print("æµ‹è¯•æµå¼ç”Ÿæˆå¯è§†åŒ–")
    print("=" * 50 + "\n")

    class MockStreamer:
        """æ¨¡æ‹Ÿæµå¼ç”Ÿæˆå™¨"""

        def __iter__(self):
            import random

            words = ["åˆ†æ", "å¸‚åœº", "è¶‹åŠ¿", "æ•°æ®", "å»ºè®®", "ç­–ç•¥", "é£é™©", "æ”¶ç›Š"]
            for i in range(20):
                time.sleep(random.uniform(0.05, 0.2))  # æ¨¡æ‹Ÿç”Ÿæˆå»¶è¿Ÿ
                yield random.choice(words)

    mock_streamer = MockStreamer()
    result = visualizer.visualize_generation(
        mock_streamer, max_new_tokens=20, model_name="FIN-R1"
    )

    print(f"\nç”Ÿæˆç»“æœ: {result}")
