"""
è´å¶æ–¯ä¼˜åŒ–å™¨ - ç”¨äºç­–ç•¥å‚æ•°ä¼˜åŒ–
ä½¿ç”¨è´å¶æ–¯ä¼˜åŒ–è‡ªåŠ¨å¯»æ‰¾æœ€ä¼˜å‚æ•°ç»„åˆ
"""

from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Callable, Dict, List, Optional, Tuple

import numpy as np
from scipy.optimize import minimize
from scipy.stats import norm
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import Matern

from common.logging_system import setup_logger

LOGGER = setup_logger("bayesian_optimizer")


@dataclass
class OptimizationResult:
    """ä¼˜åŒ–ç»“æœ"""

    best_params: Dict[str, float]
    best_score: float
    iterations: int
    param_history: List[Dict[str, float]]
    score_history: List[float]
    convergence_reached: bool


class BayesianOptimizer:
    """è´å¶æ–¯ä¼˜åŒ–å™¨

    ä½¿ç”¨é«˜æ–¯è¿‡ç¨‹å›å½’å’Œé‡‡é›†å‡½æ•°è‡ªåŠ¨å¯»æ‰¾æœ€ä¼˜å‚æ•°
    æ¯”ç½‘æ ¼æœç´¢æ›´é«˜æ•ˆï¼Œæ¯”éšæœºæœç´¢æ›´å‡†ç¡®
    """

    def __init__(
        self,
        param_bounds: Dict[str, Tuple[float, float]],
        n_initial_points: int = 5,
        n_iterations: int = 25,
        acquisition: str = "ei",  # ei/ucb/poi
        random_state: Optional[int] = None,
    ):
        """åˆå§‹åŒ–è´å¶æ–¯ä¼˜åŒ–å™¨

        Args:
            param_bounds: å‚æ•°è¾¹ç•Œ {'param_name': (min, max)}
            n_initial_points: åˆå§‹éšæœºé‡‡æ ·ç‚¹æ•°
            n_iterations: ä¼˜åŒ–è¿­ä»£æ¬¡æ•°
            acquisition: é‡‡é›†å‡½æ•°ç±»å‹
            random_state: éšæœºç§å­
        """
        self.param_bounds = param_bounds
        self.param_names = list(param_bounds.keys())
        self.bounds_array = np.array([param_bounds[name] for name in self.param_names])

        self.n_initial_points = n_initial_points
        self.n_iterations = n_iterations
        self.acquisition = acquisition
        self.random_state = random_state

        # é«˜æ–¯è¿‡ç¨‹æ¨¡å‹
        kernel = Matern(nu=2.5)
        self.gp = GaussianProcessRegressor(
            kernel=kernel,
            alpha=1e-6,
            normalize_y=True,
            n_restarts_optimizer=5,
            random_state=random_state,
        )

        # å†å²è®°å½•
        self.X_observed: List[np.ndarray] = []
        self.y_observed: List[float] = []

        LOGGER.info(f"ğŸ“Š è´å¶æ–¯ä¼˜åŒ–å™¨åˆå§‹åŒ–: {len(self.param_names)}ä¸ªå‚æ•°")

    def optimize(
        self,
        objective_function: Callable[[Dict[str, float]], float],
        maximize: bool = True,
        verbose: bool = True,
    ) -> OptimizationResult:
        """æ‰§è¡Œè´å¶æ–¯ä¼˜åŒ–

        Args:
            objective_function: ç›®æ ‡å‡½æ•°ï¼Œæ¥å—å‚æ•°å­—å…¸ï¼Œè¿”å›è¯„åˆ†
            maximize: True=æœ€å¤§åŒ–ï¼ŒFalse=æœ€å°åŒ–
            verbose: æ˜¯å¦æ‰“å°è¿›åº¦

        Returns:
            ä¼˜åŒ–ç»“æœ
        """
        try:
            np.random.seed(self.random_state)

            LOGGER.info(f"ğŸš€ å¼€å§‹è´å¶æ–¯ä¼˜åŒ–: {self.n_iterations}æ¬¡è¿­ä»£")

            # é˜¶æ®µ1: éšæœºåˆå§‹åŒ–
            if verbose:
                print(f"\né˜¶æ®µ1: éšæœºåˆå§‹åŒ– ({self.n_initial_points}ä¸ªç‚¹)")

            for i in range(self.n_initial_points):
                # éšæœºé‡‡æ ·
                x = self._random_sample()
                params = self._array_to_params(x)

                # è¯„ä¼°
                score = objective_function(params)
                if not maximize:
                    score = -score  # å†…éƒ¨ç»Ÿä¸€ä¸ºæœ€å¤§åŒ–

                self.X_observed.append(x)
                self.y_observed.append(score)

                if verbose:
                    print(f"  ç¬¬{i + 1}æ¬¡: å‚æ•°={params}, è¯„åˆ†={score:.4f}")

            # é˜¶æ®µ2: è´å¶æ–¯ä¼˜åŒ–
            if verbose:
                print(f"\né˜¶æ®µ2: è´å¶æ–¯ä¼˜åŒ– ({self.n_iterations}æ¬¡è¿­ä»£)")

            for i in range(self.n_iterations):
                # æ‹Ÿåˆé«˜æ–¯è¿‡ç¨‹
                self.gp.fit(np.array(self.X_observed), np.array(self.y_observed))

                # é€‰æ‹©ä¸‹ä¸€ä¸ªé‡‡æ ·ç‚¹
                x_next = self._propose_location()
                params_next = self._array_to_params(x_next)

                # è¯„ä¼°
                score_next = objective_function(params_next)
                if not maximize:
                    score_next = -score_next

                # è®°å½•
                self.X_observed.append(x_next)
                self.y_observed.append(score_next)

                # å½“å‰æœ€ä½³
                best_idx = np.argmax(self.y_observed)
                best_score = self.y_observed[best_idx]
                best_params = self._array_to_params(self.X_observed[best_idx])

                if verbose:
                    print(
                        f"  ç¬¬{i + 1}æ¬¡: "
                        f"å‚æ•°={params_next}, "
                        f"è¯„åˆ†={score_next:.4f}, "
                        f"å½“å‰æœ€ä½³={best_score:.4f}"
                    )

            # æ•´ç†ç»“æœ
            best_idx = np.argmax(self.y_observed)
            best_score_final = self.y_observed[best_idx]
            best_params_final = self._array_to_params(self.X_observed[best_idx])

            if not maximize:
                best_score_final = -best_score_final
                y_observed_display = [-y for y in self.y_observed]
            else:
                y_observed_display = self.y_observed.copy()

            result = OptimizationResult(
                best_params=best_params_final,
                best_score=best_score_final,
                iterations=len(self.X_observed),
                param_history=[self._array_to_params(x) for x in self.X_observed],
                score_history=y_observed_display,
                convergence_reached=self._check_convergence(),
            )

            LOGGER.info(f"âœ… ä¼˜åŒ–å®Œæˆ: æœ€ä½³è¯„åˆ†={best_score_final:.4f}")
            LOGGER.info(f"   æœ€ä½³å‚æ•°: {best_params_final}")

            return result

        except Exception as e:
            LOGGER.error(f"âŒ è´å¶æ–¯ä¼˜åŒ–å¤±è´¥: {e}", exc_info=True)
            raise

    def _random_sample(self) -> np.ndarray:
        """éšæœºé‡‡æ ·ä¸€ä¸ªå‚æ•°ç‚¹"""
        x = np.random.uniform(self.bounds_array[:, 0], self.bounds_array[:, 1])
        return x

    def _array_to_params(self, x: np.ndarray) -> Dict[str, float]:
        """å°†æ•°ç»„è½¬æ¢ä¸ºå‚æ•°å­—å…¸"""
        return {name: float(x[i]) for i, name in enumerate(self.param_names)}

    def _params_to_array(self, params: Dict[str, float]) -> np.ndarray:
        """å°†å‚æ•°å­—å…¸è½¬æ¢ä¸ºæ•°ç»„"""
        return np.array([params[name] for name in self.param_names])

    def _propose_location(self) -> np.ndarray:
        """æå‡ºä¸‹ä¸€ä¸ªé‡‡æ ·ç‚¹"""

        # å®šä¹‰é‡‡é›†å‡½æ•°
        def acquisition_function(x):
            x = x.reshape(1, -1)

            # é¢„æµ‹å‡å€¼å’Œæ ‡å‡†å·®
            mu, sigma = self.gp.predict(x, return_std=True)
            mu = mu[0]
            sigma = sigma[0]

            # å½“å‰æœ€ä½³å€¼
            mu_best = np.max(self.y_observed)

            if self.acquisition == "ei":
                # Expected Improvement (æœŸæœ›æ”¹è¿›)
                with np.errstate(divide="warn"):
                    imp = mu - mu_best
                    Z = imp / sigma if sigma > 0 else 0
                    ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z) if sigma > 0 else 0
                return -ei  # minimizeä¼šæœ€å°åŒ–ï¼Œæ‰€ä»¥å–è´Ÿ

            elif self.acquisition == "ucb":
                # Upper Confidence Bound (ç½®ä¿¡ä¸Šç•Œ)
                kappa = 2.0  # æ¢ç´¢-åˆ©ç”¨æƒè¡¡å‚æ•°
                ucb = mu + kappa * sigma
                return -ucb

            elif self.acquisition == "poi":
                # Probability of Improvement (æ”¹è¿›æ¦‚ç‡)
                xi = 0.01  # æ¢ç´¢å‚æ•°
                with np.errstate(divide="warn"):
                    imp = mu - mu_best - xi
                    Z = imp / sigma if sigma > 0 else 0
                    poi = norm.cdf(Z) if sigma > 0 else 0
                return -poi

            else:
                raise ValueError(f"æœªçŸ¥çš„é‡‡é›†å‡½æ•°: {self.acquisition}")

        # å¤šèµ·ç‚¹ä¼˜åŒ–é‡‡é›†å‡½æ•°
        x_tries = np.random.uniform(
            self.bounds_array[:, 0],
            self.bounds_array[:, 1],
            size=(100, len(self.param_names)),
        )

        ys = [acquisition_function(x) for x in x_tries]
        x_best = x_tries[np.argmin(ys)]

        # ä¼˜åŒ–
        res = minimize(
            acquisition_function, x0=x_best, bounds=self.bounds_array, method="L-BFGS-B"
        )

        return res.x

    def _check_convergence(self, window: int = 5, threshold: float = 1e-4) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ”¶æ•›

        Args:
            window: æ£€æŸ¥æœ€è¿‘Næ¬¡è¿­ä»£
            threshold: æ”¶æ•›é˜ˆå€¼

        Returns:
            æ˜¯å¦æ”¶æ•›
        """
        if len(self.y_observed) < window:
            return False

        recent_scores = self.y_observed[-window:]
        score_range = max(recent_scores) - min(recent_scores)

        return score_range < threshold

    def predict(self, params: Dict[str, float]) -> Tuple[float, float]:
        """é¢„æµ‹ç»™å®šå‚æ•°çš„è¯„åˆ†ï¼ˆå‡å€¼å’Œæ ‡å‡†å·®ï¼‰

        Args:
            params: å‚æ•°å­—å…¸

        Returns:
            (å‡å€¼, æ ‡å‡†å·®)
        """
        if not self.X_observed:
            raise ValueError("å°šæœªè¿›è¡Œä»»ä½•è§‚æµ‹")

        x = self._params_to_array(params).reshape(1, -1)
        mu, sigma = self.gp.predict(x, return_std=True)

        return float(mu[0]), float(sigma[0])

    def get_importance_scores(self) -> Dict[str, float]:
        """è®¡ç®—å‚æ•°é‡è¦æ€§è¯„åˆ†

        é€šè¿‡åˆ†æå‚æ•°å¯¹ç›®æ ‡å‡½æ•°çš„å½±å“æ¥è¯„ä¼°é‡è¦æ€§

        Returns:
            å‚æ•°é‡è¦æ€§å­—å…¸
        """
        if len(self.X_observed) < 10:
            LOGGER.warning("âš ï¸ è§‚æµ‹ç‚¹è¿‡å°‘ï¼Œé‡è¦æ€§è¯„åˆ†å¯èƒ½ä¸å‡†ç¡®")

        importance = {}

        X = np.array(self.X_observed)
        y = np.array(self.y_observed)

        for i, param_name in enumerate(self.param_names):
            # è®¡ç®—è¯¥å‚æ•°ä¸ç›®æ ‡å‡½æ•°çš„ç›¸å…³æ€§
            correlation = np.corrcoef(X[:, i], y)[0, 1]
            importance[param_name] = abs(correlation)

        # å½’ä¸€åŒ–
        total = sum(importance.values())
        if total > 0:
            importance = {k: v / total for k, v in importance.items()}

        return importance


def optimize_strategy_params(
    backtest_function: Callable[[Dict], float],
    param_ranges: Dict[str, Tuple[float, float]],
    metric: str = "sharpe_ratio",
    n_iterations: int = 25,
) -> OptimizationResult:
    """ä¼˜åŒ–ç­–ç•¥å‚æ•°çš„ä¾¿æ·å‡½æ•°

    Args:
        backtest_function: å›æµ‹å‡½æ•°ï¼Œæ¥å—å‚æ•°è¿”å›å›æµ‹ç»“æœ
        param_ranges: å‚æ•°èŒƒå›´
        metric: ä¼˜åŒ–æŒ‡æ ‡
        n_iterations: è¿­ä»£æ¬¡æ•°

    Returns:
        ä¼˜åŒ–ç»“æœ
    """

    def objective(params: Dict[str, float]) -> float:
        """ç›®æ ‡å‡½æ•°"""
        result = backtest_function(params)
        return result.get(metric, 0.0)

    optimizer = BayesianOptimizer(param_bounds=param_ranges, n_iterations=n_iterations)

    return optimizer.optimize(objective, maximize=True)


# ç¤ºä¾‹ç”¨æ³•
if __name__ == "__main__":
    # ç¤ºä¾‹ï¼šä¼˜åŒ–LSTMç­–ç•¥å‚æ•°
    def mock_backtest(params: Dict[str, float]) -> float:
        """æ¨¡æ‹Ÿå›æµ‹å‡½æ•°"""
        # å‡è®¾å¤æ™®æ¯”ç‡ä¸å‚æ•°çš„å…³ç³»
        sharpe = (
            1.5
            - 0.1 * (params["stop_loss"] + 0.05) ** 2
            + 0.05 * np.log(params["ma_period"] / 10)
            - 0.02 * (params["lstm_hidden"] - 128) ** 2 / 10000
        )
        # æ·»åŠ å™ªå£°
        sharpe += np.random.normal(0, 0.1)
        return sharpe

    # å®šä¹‰å‚æ•°èŒƒå›´
    param_bounds = {
        "stop_loss": (-0.10, -0.02),  # æ­¢æŸ -10% åˆ° -2%
        "ma_period": (5, 60),  # ç§»åŠ¨å¹³å‡å‘¨æœŸ
        "lstm_hidden": (64, 256),  # LSTMéšè—å±‚å¤§å°
    }

    # æ‰§è¡Œä¼˜åŒ–
    optimizer = BayesianOptimizer(param_bounds, n_iterations=20)
    result = optimizer.optimize(mock_backtest, maximize=True, verbose=True)

    print("\n" + "=" * 60)
    print("ä¼˜åŒ–å®Œæˆï¼")
    print(f"æœ€ä½³å‚æ•°: {result.best_params}")
    print(f"æœ€ä½³è¯„åˆ†: {result.best_score:.4f}")
    print(f"è¿­ä»£æ¬¡æ•°: {result.iterations}")
    print(f"æ˜¯å¦æ”¶æ•›: {result.convergence_reached}")

    # å‚æ•°é‡è¦æ€§
    importance = optimizer.get_importance_scores()
    print("\nå‚æ•°é‡è¦æ€§:")
    for param, score in sorted(importance.items(), key=lambda x: x[1], reverse=True):
        print(f"  {param}: {score:.3f}")
