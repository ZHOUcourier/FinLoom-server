"""
æ¨¡å‹æ¨ç†è¯Šæ–­å·¥å…·
å¸®åŠ©æ’æŸ¥æ¨¡å‹æ¨ç†å¡ä½æˆ–é€Ÿåº¦æ…¢çš„é—®é¢˜
"""

import os
import time

import psutil
from rich.console import Console
from rich.panel import Panel
from rich.table import Table

console = Console()


def check_system_resources():
    """æ£€æŸ¥ç³»ç»Ÿèµ„æº"""
    console.print("\n[bold cyan]ğŸ“Š ç³»ç»Ÿèµ„æºæ£€æŸ¥[/bold cyan]\n")

    table = Table(show_header=True)
    table.add_column("èµ„æº", style="cyan")
    table.add_column("çŠ¶æ€", style="white")
    table.add_column("è¯´æ˜", style="yellow")

    # CPU
    cpu_percent = psutil.cpu_percent(interval=1)
    cpu_count = psutil.cpu_count()
    table.add_row(
        "CPU",
        f"{cpu_percent}% ({cpu_count} cores)",
        "âœ… OK" if cpu_percent < 90 else "âš ï¸ High",
    )

    # å†…å­˜
    mem = psutil.virtual_memory()
    table.add_row(
        "å†…å­˜",
        f"{mem.percent}% ({mem.available / 1024**3:.1f}GB free)",
        "âœ… OK" if mem.percent < 90 else "âš ï¸ High",
    )

    # ç£ç›˜
    disk = psutil.disk_usage("/")
    table.add_row(
        "ç£ç›˜",
        f"{disk.percent}% ({disk.free / 1024**3:.1f}GB free)",
        "âœ… OK" if disk.percent < 90 else "âš ï¸ High",
    )

    console.print(table)


def check_pytorch():
    """æ£€æŸ¥PyTorché…ç½®"""
    console.print("\n[bold cyan]ğŸ”¥ PyTorché…ç½®æ£€æŸ¥[/bold cyan]\n")

    try:
        import torch

        table = Table(show_header=True)
        table.add_column("é¡¹ç›®", style="cyan")
        table.add_column("å€¼", style="white")

        table.add_row("PyTorchç‰ˆæœ¬", torch.__version__)
        table.add_row(
            "CUDAå¯ç”¨", "âœ… Yes" if torch.cuda.is_available() else "âŒ No (ä½¿ç”¨CPU)"
        )

        if torch.cuda.is_available():
            table.add_row("CUDAç‰ˆæœ¬", torch.version.cuda)
            table.add_row("GPUæ•°é‡", str(torch.cuda.device_count()))
            table.add_row("å½“å‰GPU", torch.cuda.get_device_name(0))
        else:
            table.add_row("è®¾å¤‡", "CPU (è¾ƒæ…¢)")
            table.add_row("âš ï¸ æç¤º", "ä½¿ç”¨CPUæ¨ç†ä¼šå¾ˆæ…¢ï¼Œå»ºè®®ä½¿ç”¨GPU")

        table.add_row("çº¿ç¨‹æ•°", str(torch.get_num_threads()))

        console.print(table)

    except ImportError:
        console.print("[red]âŒ PyTorchæœªå®‰è£…[/red]")


def test_model_loading():
    """æµ‹è¯•æ¨¡å‹åŠ è½½"""
    console.print("\n[bold cyan]ğŸ“¦ æµ‹è¯•æ¨¡å‹åŠ è½½[/bold cyan]\n")

    model_path = ".Fin-R1"

    if not os.path.exists(model_path):
        console.print(f"[red]âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}[/red]")
        console.print("[yellow]ğŸ’¡ è¯·ç¡®ä¿ FIN-R1 æ¨¡å‹å·²ä¸‹è½½åˆ°å½“å‰ç›®å½•[/yellow]")
        return False

    try:
        import torch
        from transformers import AutoModelForCausalLM, AutoTokenizer

        console.print(f"âœ“ æ¨¡å‹è·¯å¾„å­˜åœ¨: {model_path}")

        # æµ‹è¯•åˆ†è¯å™¨
        console.print("â³ åŠ è½½åˆ†è¯å™¨...")
        start = time.time()
        tokenizer = AutoTokenizer.from_pretrained(
            model_path, trust_remote_code=True, use_fast=False
        )
        console.print(f"âœ… åˆ†è¯å™¨åŠ è½½æˆåŠŸ ({time.time() - start:.2f}s)")

        # æµ‹è¯•æ¨¡å‹ï¼ˆä½¿ç”¨æœ€å°é…ç½®ï¼‰
        console.print("â³ åŠ è½½æ¨¡å‹ï¼ˆå¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼‰...")
        start = time.time()
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            trust_remote_code=True,
            torch_dtype=torch.float32,
            low_cpu_mem_usage=True,  # å‡å°‘å†…å­˜å ç”¨
        )
        model.eval()
        console.print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ ({time.time() - start:.2f}s)")

        # è®¡ç®—æ¨¡å‹å¤§å°
        param_count = sum(p.numel() for p in model.parameters())
        console.print(f"ğŸ“Š æ¨¡å‹å‚æ•°é‡: {param_count / 1e9:.2f}B")

        return True

    except Exception as e:
        console.print(f"[red]âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}[/red]")
        return False


def test_simple_generation():
    """æµ‹è¯•ç®€å•ç”Ÿæˆ"""
    console.print("\n[bold cyan]ğŸš€ æµ‹è¯•æ¨¡å‹æ¨ç†[/bold cyan]\n")

    try:
        from threading import Thread

        import torch
        from transformers import (
            AutoModelForCausalLM,
            AutoTokenizer,
            TextIteratorStreamer,
        )

        model_path = ".Fin-R1"

        console.print("â³ åˆå§‹åŒ–æ¨¡å‹...")
        tokenizer = AutoTokenizer.from_pretrained(
            model_path, trust_remote_code=True, use_fast=False
        )

        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token

        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            trust_remote_code=True,
            torch_dtype=torch.float32,
            low_cpu_mem_usage=True,
        )
        model.eval()

        # ç®€å•æµ‹è¯•
        console.print("â³ æµ‹è¯•ç”Ÿæˆï¼ˆéæµå¼ï¼‰...")
        test_input = "ä½ å¥½"
        inputs = tokenizer(test_input, return_tensors="pt")

        start = time.time()
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=5,  # åªç”Ÿæˆ5ä¸ªtokenæµ‹è¯•
                do_sample=False,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
            )
        elapsed = time.time() - start

        result = tokenizer.decode(outputs[0], skip_special_tokens=True)
        console.print(f"âœ… éæµå¼ç”ŸæˆæˆåŠŸ ({elapsed:.2f}s)")
        console.print(f"   è¾“å…¥: {test_input}")
        console.print(f"   è¾“å‡º: {result}")
        console.print(f"   é€Ÿç‡: {5 / elapsed:.2f} tokens/s")

        # æµ‹è¯•æµå¼ç”Ÿæˆ
        console.print("\nâ³ æµ‹è¯•ç”Ÿæˆï¼ˆæµå¼ï¼‰...")
        streamer = TextIteratorStreamer(
            tokenizer,
            skip_prompt=True,
            skip_special_tokens=True,
            timeout=10.0,  # 10ç§’è¶…æ—¶
        )

        generation_kwargs = dict(
            **inputs,
            streamer=streamer,
            max_new_tokens=10,
            do_sample=False,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )

        thread = Thread(target=model.generate, kwargs=generation_kwargs)
        start = time.time()
        thread.start()

        generated_text = ""
        token_count = 0
        try:
            for text in streamer:
                generated_text += text
                token_count += 1
                console.print(f"  Token #{token_count}: {text}", end="")
        except TimeoutError:
            console.print("\n[red]âŒ æµå¼ç”Ÿæˆè¶…æ—¶ï¼[/red]")
            return False

        thread.join(timeout=20)
        elapsed = time.time() - start

        console.print(f"\nâœ… æµå¼ç”ŸæˆæˆåŠŸ ({elapsed:.2f}s)")
        console.print(f"   ç”Ÿæˆäº† {token_count} tokens")
        console.print(f"   é€Ÿç‡: {token_count / elapsed:.2f} tokens/s")

        if token_count == 0:
            console.print("[yellow]âš ï¸ è­¦å‘Šï¼šæ²¡æœ‰ç”Ÿæˆä»»ä½•tokenï¼[/yellow]")
            return False

        return True

    except Exception as e:
        console.print(f"[red]âŒ æ¨ç†æµ‹è¯•å¤±è´¥: {e}[/red]")
        import traceback

        console.print(traceback.format_exc())
        return False


def main():
    """ä¸»å‡½æ•°"""
    console.print(
        Panel.fit(
            "[bold cyan]ğŸ” FIN-R1 æ¨¡å‹æ¨ç†è¯Šæ–­å·¥å…·[/bold cyan]\n"
            "å¸®åŠ©æ’æŸ¥æ¨¡å‹æ¨ç†å¡ä½æˆ–é€Ÿåº¦æ…¢çš„é—®é¢˜",
            border_style="cyan",
        )
    )

    # 1. æ£€æŸ¥ç³»ç»Ÿèµ„æº
    check_system_resources()

    # 2. æ£€æŸ¥PyTorch
    check_pytorch()

    # 3. æµ‹è¯•æ¨¡å‹åŠ è½½
    model_loaded = test_model_loading()

    if not model_loaded:
        console.print("\n[red]âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œæ— æ³•ç»§ç»­æµ‹è¯•[/red]")
        return

    # 4. æµ‹è¯•æ¨ç†
    inference_ok = test_simple_generation()

    # æ€»ç»“
    console.print("\n" + "=" * 60)
    if inference_ok:
        console.print("[bold green]âœ… è¯Šæ–­å®Œæˆï¼šæ¨¡å‹å·¥ä½œæ­£å¸¸ï¼[/bold green]")
        console.print("\nğŸ’¡ å¦‚æœå®é™…ä½¿ç”¨æ—¶è¿˜æ˜¯å¾ˆæ…¢ï¼š")
        console.print("   1. æ£€æŸ¥æ˜¯å¦åœ¨CPUä¸Šè¿è¡Œï¼ˆCPUæ¨ç†å¾ˆæ…¢ï¼‰")
        console.print("   2. å‡å°‘ max_new_tokens å‚æ•°")
        console.print("   3. è€ƒè™‘ä½¿ç”¨æ›´å°çš„æ¨¡å‹")
    else:
        console.print("[bold red]âŒ è¯Šæ–­å¤±è´¥ï¼šå‘ç°é—®é¢˜ï¼[/bold red]")
        console.print("\nğŸ”§ å¯èƒ½çš„è§£å†³æ–¹æ¡ˆï¼š")
        console.print("   1. ç¡®ä¿æ¨¡å‹æ–‡ä»¶å®Œæ•´ä¸‹è½½")
        console.print(
            "   2. æ£€æŸ¥ transformers ç‰ˆæœ¬ï¼špip install --upgrade transformers"
        )
        console.print("   3. æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„å†…å­˜")
        console.print("   4. å°è¯•é‡æ–°ä¸‹è½½æ¨¡å‹")
    console.print("=" * 60 + "\n")


if __name__ == "__main__":
    main()
